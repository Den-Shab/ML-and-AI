{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5329 entries, 1186 to 7270\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        5329 non-null   int64 \n",
      " 1   keyword   5285 non-null   object\n",
      " 2   location  3569 non-null   object\n",
      " 3   text      5329 non-null   object\n",
      " 4   target    5329 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 249.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.columns].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.columns].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5329 entries, 1186 to 7270\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        5329 non-null   int64 \n",
      " 1   keyword   5329 non-null   object\n",
      " 2   location  5329 non-null   object\n",
      " 3   text      5329 non-null   object\n",
      " 4   target    5329 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 249.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2284 entries, 2644 to 6753\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        2284 non-null   int64 \n",
      " 1   keyword   2284 non-null   object\n",
      " 2   location  2284 non-null   object\n",
      " 3   text      2284 non-null   object\n",
      " 4   target    2284 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 107.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3024\n",
       "1    2305\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts() # распределение классов приблизительно равное, с небольшим преобладание отрицательного класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.keyword.value_counts().head(11)\n",
    "a = a.keys().delete(0) #удаляем ключевое слово ''\n",
    "data = train[train.keyword.isin(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x167cf395970>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1050.38x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[train['keyword'] =='damage'].value_counts('target')\n",
    "ll = [1] * len(data.target)\n",
    "import seaborn as sns\n",
    "sns.catplot(x='keyword', y=ll, estimator=sum, hue='target', kind=\"bar\", data=data, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"keyword\"]+' '+train[\"location\"]+' '+train[\"text\"]\n",
    "train = train.drop(['id','keyword','location'], axis=1)\n",
    "test[\"text\"] = test[\"keyword\"]+' '+test[\"location\"]+' '+test[\"text\"]\n",
    "test = test.drop(['id','keyword','location'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x18455 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(train.text)\n",
    "bow\n",
    "#Видно, что матрица получилось достаточно больших размеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(vec):\n",
    "    numb = []\n",
    "    punc = []\n",
    "    has = []\n",
    "    for i in vec.vocabulary_.keys():\n",
    "        if re.search('\\d+', i):\n",
    "            numb.append(i)\n",
    "        if re.search('[^\\w\\s]|_', i):\n",
    "            punc.append(i)\n",
    "        if re.search('(@|#).*', i):\n",
    "            has.append(i)\n",
    "    return numb, punc, has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812\n",
      "315\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a,b,c = func1(vec)\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x19670 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94563 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tw = TweetTokenizer()\n",
    "vec1 = CountVectorizer(ngram_range=(1, 1), tokenizer=tw.tokenize)\n",
    "bow1 = vec1.fit_transform(train.text)\n",
    "bow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3939\n",
      "7354\n",
      "3155\n"
     ]
    }
   ],
   "source": [
    "a,b,c = func1(vec1)\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Можно заметить, что при использовании TweetTokenizer мы получили более 3000 хэштегов и упоминаний, в то время как стандартный CountVectorizer не определил ни одного. Также в разы увеличилось количество токенов, содержащих знаки препинания(стандартный токенизатор отбрасывал все знаки перпинания, кроме \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(string):\n",
    "    string = string.lower()\n",
    "    tw = TweetTokenizer()\n",
    "    lst = tw.tokenize(string)\n",
    "    i = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] in nltk.corpus.stopwords.words('english'):\n",
    "            lst.remove(lst[i])\n",
    "            i -= 1\n",
    "        elif re.search('[^a-z]+', lst[i]):\n",
    "            if not re.search('([^\\w\\s]|_)*[\\(\\)]+([^\\w\\s]|_)*', lst[i]) and not re.search('#[a-z]+', lst[i]):\n",
    "                lst.remove(lst[i])\n",
    "                i -= 1\n",
    "        i += 1\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    lst_stemmed = [stemmer.stem(w) for w in lst]\n",
    "    return lst_stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridg', 'ash', 'australia', 'collaps', 'trent', 'bridg', 'among', 'worst', 'histori', 'england', 'bundl', 'australia']\n",
      "['hail', 'carol', 'stream', 'illinoi', 'great', 'michigan', 'techniqu', 'camp', 'thank', '#goblu', '#wrestleon']\n",
      "['polic', 'houston', 'cnn', 'tennesse', 'movi', 'theater', 'shoot', 'suspect', 'kill', 'polic']\n",
      "['riot', 'still', 'riot', 'coupl', 'hour', 'left', 'class']\n",
      "['wound', 'lake', 'highland', 'crack', 'path', 'wipe', 'morn', 'beach', 'run', 'surfac', 'wound', 'left', 'elbow', 'right', 'knee']\n",
      "['airplan', 'somewher', 'expert', 'franc', 'begin', 'examin', 'airplan', 'debri', 'found', 'reunion', 'island', 'french', 'air', 'accid', 'expert', '#mlb']\n",
      "['bloodi', 'isol', 'citi', 'world', 'perth', 'came', 'kill', 'indian', 'fun', 'video', 'smirk', 'remorseless', 'pakistani', 'killer', 'show', 'boast']\n",
      "['burn', 'except', 'idk', 'realli', 'burn']\n",
      "['destroy', '(', 'ask', ')', 'destroy', 'hous']\n",
      "['wound', 'maracay', 'nirgua', 'venezuela', 'polic', 'offic', 'wound', 'suspect', 'dead', 'exchang', 'shot']\n"
     ]
    }
   ],
   "source": [
    "a = train.head(10)\n",
    "for text in a.items():\n",
    "    for i in text[1]:\n",
    "        if type(i) == int:\n",
    "            break\n",
    "        print(custom_tokenizer(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "train_tok = vec2.fit_transform(train.text)\n",
    "test_tok = vec2.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7540279269602578"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433333333333335"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec_tf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=custom_tokenizer)\n",
    "train_tok = vec_tf.fit_transform(train.text)\n",
    "test_tok = vec_tf.transform(test.text)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433333333333335"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=custom_tokenizer, max_df=0.9)\n",
    "train_tok = vec_tf.fit_transform(train.text)\n",
    "test_tok = vec_tf.transform(test.text)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7444690265486725"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(ngram_range=(1, 1), tokenizer=custom_tokenizer, max_df=0.9, min_df=0.0005)\n",
    "train_tok = vec_tf.fit_transform(train.text)\n",
    "test_tok = vec_tf.transform(test.text)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7191513121161363"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec_hash = HashingVectorizer(ngram_range=(1, 1), tokenizer=custom_tokenizer, n_features=5000)\n",
    "train_tok = vec_hash.fit_transform(train.text)\n",
    "test_tok = vec_hash.transform(test.text)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат показал CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7540279269602578"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2 = CountVectorizer(ngram_range=(1, 1), tokenizer=custom_tokenizer)\n",
    "train_tok = vec2.fit_transform(train.text)\n",
    "test_tok = vec2.transform(test.text)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tok, train.target)\n",
    "f1_score(test.target, lr.predict(test_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "изменения параметра ngram_range в CountVectorizer, а также границ df а TfDfVectorizer, не привели к увелечению f1_score относительно изначального результата"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
